{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to build a simple ReAct AI Agentic application for everyday DevOps tasks using REST APIs\n",
    "\n",
    "This notebook demonstrates how to create a simple ReAct AI agent application that performs simple tasks such as retrieving GitHub repo descriptions or labels through standard REST APIs. The app uses LangGraph to define the agentic workflow. We'll also showcase how the ReAct (Reasoning and Acting) framework lets the agent decide when and how to use tools effectively.\n",
    "\n",
    "![](docs/ai-agent.svg)\n",
    "\n",
    "The workflow follows a loop in which the agent reasons whether to call the tools and if so, the tools are executed. The results are fed back to the agent for further decision-making.\n",
    "\n",
    "The process works like this:\n",
    "\n",
    "- Step 1: Interact with the app, prompting the agent (LLM) to decide whether it should use a tool like GitHub API.\n",
    "- Step 2: If the agent recommends taking an action (e.g., using the tool), it calls the API and retrieves results.\n",
    "- Step 3: The agent evaluates the results, and either takes further action or provides a final response.\n",
    "\n",
    "Let's walk through the components depicted in the diagram.\n",
    "\n",
    "## Understanding the Components:\n",
    "\n",
    "1. **FastAPI Server:** This server acts as the entry point for HTTP requests. We will use the command-line curl utility to test our AI Agent FastAPI server.\n",
    "2. **LangGraph State Machine:** At the core of our sample application is the LangGraph state machine, which maintains the workflow for interacting with multiple agents and tools. LangGraph defines the workflow and ensures the state is managed correctly as the AI agent proceeds through its reasoning steps.\n",
    "3. **Agent (LLM Calls):** The AI agent, powered by a Large Language Model (LLM), makes decisions based on the input provided. The agent is central to the ReAct (Reasoning and Acting) architecture, where it continuously evaluates its actions, deciding whether it needs to call a particular tool, like make a GitHub API request or return the final answer to the user.\n",
    "4. **Tools Integration:** The agent uses various tools to fulfill its tasks. For instance, when tasked with fetching data about a GitHub repository description, the agent invokes the GitHub API as one of its \"tools.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Implementation\n",
    "Here is the code implementation for building a simple ReAct AI agent to retrieve information from GitHub repositories using the GitHub API. The application uses FastAPI to interact with external clients, LangGraph to manage workflows, and a Large Language Model (LLM) to reason and make decisions.\n",
    "\n",
    "_Code Disclaimer:_ The code provided in this blog is for educational and informational purposes only. While every effort is made to ensure the code is functional and accurate, it is provided \"as-is\" without any guarantees. Use the code at your own risk. The author is not responsible for any damage or data loss caused by implementing the code. Always review and test code in a safe environment before using it in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting Up the Environment\n",
    "\n",
    "- Python3 and pip are prerequisites for proceeding. \n",
    "- Create Python virtual env\n",
    "\n",
    "```\n",
    "python -m venv venv\n",
    "source venv/bin/activate\n",
    "```\n",
    "\n",
    "- Install the python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastapi[standard] langgraph langchain_openai langchain_core langserve[server] requests ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create the necessary tools for the Agent to invoke based on input\n",
    "In this section, we create the tools the agents will invoke based on their reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function call to get the GitHub repo description based on the repo name and GitHub org name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools\n",
    "@tool\n",
    "def get_github_repo_description(repo_name: str, org_name: str) -> str:\n",
    "  \"\"\"\n",
    "  Fetches the description of a GitHub repository.\n",
    "\n",
    "  Args:\n",
    "  repo_name (str): The name of the repository.\n",
    "  org_name (str): The name of the organization or user that owns the repository.\n",
    "\n",
    "  Returns:\n",
    "  str: The description of the repository if available, otherwise a message indicating\n",
    "      that no description is available or an error message if the request fails.\n",
    "\n",
    "  Raises:\n",
    "  requests.exceptions.RequestException: If there is an issue with the HTTP request.\n",
    "  \"\"\"\n",
    "  url = f\"https://api.github.com/repos/{org_name}/{repo_name}\"\n",
    "  headers = {\n",
    "    \"Authorization\": f\"token {os.getenv('GITHUB_TOKEN')}\"\n",
    "  }\n",
    "  response = requests.get(url, headers=headers)\n",
    "  if response.status_code == 200:\n",
    "    repo_info = response.json()\n",
    "    return repo_info['description'] if repo_info['description'] else f\"No description available for {repo_name} in {org_name}\"\n",
    "  else:\n",
    "    return f\"Failed to fetch description for {repo_name} in {org_name}. Status code: {response.status_code}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function call to get GitHub repo topics based on repo name and GitHub org name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_github_repo_topics(repo_name: str, org_name: str) -> str:\n",
    "  \"\"\"\n",
    "  Fetches the topics of a GitHub repository.\n",
    "\n",
    "  Args:\n",
    "  repo_name (str): The name of the repository.\n",
    "  org_name (str): The name of the organization or user that owns the repository.\n",
    "\n",
    "  Returns:\n",
    "  str: A comma-separated string of topics if available, otherwise a message indicating\n",
    "      that no topics are available or an error message if the request fails.\n",
    "\n",
    "  Raises:\n",
    "  requests.exceptions.RequestException: If there is an issue with the HTTP request.\n",
    "  \"\"\"\n",
    "  url = f\"https://api.github.com/repos/{org_name}/{repo_name}/topics\"\n",
    "  headers = {\n",
    "    \"Authorization\": f\"token {os.getenv('GITHUB_TOKEN')}\",\n",
    "    \"Accept\": \"application/vnd.github.mercy-preview+json\"\n",
    "  }\n",
    "  response = requests.get(url, headers=headers)\n",
    "  if response.status_code == 200:\n",
    "    repo_info = response.json()\n",
    "    topics = repo_info.get('names', [])\n",
    "    return \", \".join(topics) if topics else f\"No topics available for {repo_name} in {org_name}\"\n",
    "  else:\n",
    "    return f\"Failed to fetch topics for {repo_name} in {org_name}. Status code: {response.status_code}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LLM_MODEL_NAME\"] = \"openai\"\n",
    "os.environ[\"OPENAI_MODEL\"] = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ask for user input to get OPENAI_API_KEY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ask for user input to get GITHUB_TOKEN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"GITHUB_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define the SimpleAIAgent Python Class\n",
    "In this section, we define the SimpleAIClass class, which powers our agent's reasoning capabilities. Based on the environment variable, the user can choose to either use OpenAI or Azure OpenAI. Default is OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, AzureChatOpenAI\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition, ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAIAgent():\n",
    "  '''\n",
    "  Simple AI Agent is a class that initializes and configures an AI assistant using LLMs.\n",
    "  '''\n",
    "  def __init__(self, thread_id: str):\n",
    "    # Specify a thread\n",
    "    self.config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "    # Initialize LLMFactory with the desired model name\n",
    "    model_name = os.getenv(\"LLM_MODEL_NAME\", \"openai\")\n",
    "\n",
    "    if model_name == \"azure_openai\":\n",
    "      deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "      api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "      llm = AzureChatOpenAI(\n",
    "        azure_deployment=deployment,\n",
    "        api_version=api_version,\n",
    "        temperature=0,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        max_retries=2\n",
    "      )\n",
    "    elif model_name == \"openai\":\n",
    "      llm = ChatOpenAI(\n",
    "        model=os.getenv(\"OPENAI_MODEL\"),\n",
    "        temperature=0,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        max_retries=2\n",
    "      )\n",
    "\n",
    "    tools = [\n",
    "      get_github_repo_description,\n",
    "      get_github_repo_topics\n",
    "    ]\n",
    "\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "    # System message\n",
    "    sys_msg = SystemMessage(content=(\n",
    "      \"You are a helpful Assistant tasked with performing tasks.\\n\"\n",
    "      \"You can assist with github repo operations \\n\"\n",
    "    ), pretty_repr=True)\n",
    "\n",
    "    # Node\n",
    "    def assistant(state: MessagesState):\n",
    "      return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n",
    "\n",
    "    # Graph\n",
    "    self.builder = StateGraph(MessagesState)\n",
    "\n",
    "    # Define nodes: They do the actual work\n",
    "    self.builder.add_node(\"simple_ai_agent\", assistant)\n",
    "    self.builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "    # Define edges: these determine how the control flow moves\n",
    "    self.builder.add_edge(START, \"simple_ai_agent\")\n",
    "\n",
    "    # Add conditional edges\n",
    "    self.builder.add_conditional_edges(\n",
    "      \"simple_ai_agent\",\n",
    "      tools_condition,\n",
    "      \"tools\"\n",
    "    )\n",
    "\n",
    "    # Add an edge to the tool node\n",
    "    self.builder.add_edge(\"tools\", \"simple_ai_agent\")\n",
    "\n",
    "    checkpointer = MemorySaver()\n",
    "    self.react_graph_memory = self.builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "    self.react_graph_memory = self.builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "  def interact(self, human_message:str):\n",
    "    try:\n",
    "      # Specify an input\n",
    "      messages = [HumanMessage(content=human_message)]\n",
    "\n",
    "      # return messages\n",
    "      messages = self.react_graph_memory.invoke({\"messages\": messages}, self.config)\n",
    "\n",
    "      # print messages\n",
    "      for message in messages['messages']:\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"Type: {type(message).__name__}, Content: {message.content}\")\n",
    "        if message.additional_kwargs and 'tool_calls' in message.additional_kwargs:\n",
    "            print(f\"Tool Call ID: {message.additional_kwargs['tool_calls'][0]['id']}, Name: {message.additional_kwargs['tool_calls'][0]['function']['name']}, Arguments: {message.additional_kwargs['tool_calls'][0]['function']['arguments']}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "      return messages['messages'][-1].content\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "\n",
    "  def create_graph_image(self, write_to_file: bool = True):\n",
    "    graph_image = self.react_graph_memory.get_graph(xray=1).draw_mermaid_png()\n",
    "    if not write_to_file:\n",
    "      return graph_image\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    filename = f\"simple_ai_agent_{timestamp}.png\"\n",
    "    with open(filename, \"wb\") as f:\n",
    "      f.write(graph_image)\n",
    "    print(f\"Graph image saved as '{filename}'. Open this file to view the graph.\")\n",
    "\n",
    "  def print_message_history(self, thread_id: str):\n",
    "    message_history = [s for s in self.react_graph_memory.get_state_history(self.config)]\n",
    "    print(f\"Message history for thread_id {thread_id}: {json.dumps(message_history, default=str, indent=2)}\")\n",
    "    print(json.dumps(message_history, default=str, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate an image of the graph to visualize our state machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "thread_id = \"0\"\n",
    "display(Image(SimpleAIAgent(thread_id).create_graph_image(write_to_file=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the ChatBotQuestion BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBotQuestion(BaseModel):\n",
    "  question: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get `uuid` for thread_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uuid() -> str:\n",
    "  unique_id = str(uuid.uuid4())\n",
    "  print(f\"Generated UUID: {unique_id}\")\n",
    "  return unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SimpleAIAgent\n",
    "agent = SimpleAIAgent(get_uuid())\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/question\")\n",
    "def ask_question(question: ChatBotQuestion):\n",
    "  \"\"\"\n",
    "  Interacts with a chatbot agent to ask a question and retrieve the response.\n",
    "\n",
    "  Args:\n",
    "    question (ChatBotQuestion): An object containing the question text and chat session ID.\n",
    "\n",
    "  Returns:\n",
    "    str: The content of the second message in the response from the chatbot agent.\n",
    "\n",
    "  Note:\n",
    "    The function returns the content of the message at index 1 because the response\n",
    "    is expected to be a list of messages, where the first message (index 0) is a HumanMessages,\n",
    "    and the second message is an AIMessage.\n",
    "  \"\"\"\n",
    "  response = agent.interact(question.question)\n",
    "  return str(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start FastAPI unicorn server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = uvicorn.Config(app, host=\"localhost\", port=8000)\n",
    "server = uvicorn.Server(config)\n",
    "await server.serve()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
